<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-hpcdocs/HPC-clusters/campus-clusters" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.5.2">
<title data-rh="true">Campus Clusters | UC Merced HPC &amp; JupyterHub Documentation</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://amirayuyue.github.io/hpc_doc_new/docs/hpcdocs/HPC-clusters/campus-clusters"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Campus Clusters | UC Merced HPC &amp; JupyterHub Documentation"><meta data-rh="true" name="description" content="HPC Clusters"><meta data-rh="true" property="og:description" content="HPC Clusters"><meta data-rh="true" property="og:image" content="https://amirayuyue.github.io/hpc_doc_new/imgs/hpc_cartoon.png"><meta data-rh="true" name="twitter:image" content="https://amirayuyue.github.io/hpc_doc_new/imgs/hpc_cartoon.png"><link data-rh="true" rel="icon" href="/hpc_doc_new/img/cirt.png"><link data-rh="true" rel="canonical" href="https://amirayuyue.github.io/hpc_doc_new/docs/hpcdocs/HPC-clusters/campus-clusters"><link data-rh="true" rel="alternate" href="https://amirayuyue.github.io/hpc_doc_new/docs/hpcdocs/HPC-clusters/campus-clusters" hreflang="en"><link data-rh="true" rel="alternate" href="https://amirayuyue.github.io/hpc_doc_new/docs/hpcdocs/HPC-clusters/campus-clusters" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/hpc_doc_new/blog/rss.xml" title="UC Merced HPC &amp; JupyterHub Documentation RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hpc_doc_new/blog/atom.xml" title="UC Merced HPC &amp; JupyterHub Documentation Atom Feed"><link rel="stylesheet" href="/hpc_doc_new/assets/css/styles.0fb755ff.css">
<script src="/hpc_doc_new/assets/js/runtime~main.1baaf25d.js" defer="defer"></script>
<script src="/hpc_doc_new/assets/js/main.ac9b5677.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hpc_doc_new/"><div class="navbar__logo"><img src="/hpc_doc_new/img/CIRT.png" alt="CIRT Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hpc_doc_new/img/CIRT.png" alt="CIRT Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Home</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hpc_doc_new/docs/category/hpc-clusters">HPC Cluster</a><a class="navbar__item navbar__link" href="/hpc_doc_new/docs/jupyter/jupyterhub">JupyterHub</a><a class="navbar__item navbar__link" href="/hpc_doc_new/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/amirayuyue/hpc_doc_new" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a class="navbar__item navbar__link" href="/hpc_doc_new/cal">Calendar</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/hpc_doc_new/docs/category/hpc-clusters">HPC Clusters</a><button aria-label="Collapse sidebar category &#x27;HPC Clusters&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/policies">University Policies and HPC Guidelines</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item tutorialSidebar"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/campus-clusters">Campus Clusters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/condo">Condo Model and Cluster Structure</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/data_sharing">Data Sharing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/data_transfer">Data Transferring</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/jobs">Running Jobs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/job_manage">Job Management</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/run_apps">Running Common Applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/get_help">HPC Support</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/hpc_news">HPC Cluster News</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/markdown-features">Markdown Features</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/hpc-qa">HPC Questions and Answers</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/hpc_doc_new/docs/category/additional-hpc-resources">Additional HPC Resources</a><button aria-label="Expand sidebar category &#x27;Additional HPC Resources&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/hpc_doc_new/docs/category/off-campus-high-performance-computing-resources">Off Campus High Performance Computing Resources</a><button aria-label="Expand sidebar category &#x27;Off Campus High Performance Computing Resources&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/hpc_doc_new/docs/category/running-jobs">Running Jobs</a><button aria-label="Expand sidebar category &#x27;Running Jobs&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/hpc_doc_new/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Expand sidebar category &#x27;Tutorial - Extras&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hpc_doc_new/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/hpc_doc_new/docs/category/hpc-clusters"><span itemprop="name">HPC Clusters</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Campus Clusters</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hpc-clusters">HPC Clusters<a href="#hpc-clusters" class="hash-link" aria-label="Direct link to HPC Clusters" title="Direct link to HPC Clusters">â€‹</a></h2>
<blockquote>
<p>Currently, UC, Merced has two clusters on site. They are maintained by the <a href="https://it.ucmerced.edu/Research-Computing-People" target="_blank" rel="noopener noreferrer">CIRT</a> team. If you have any questions, feel free to contact us <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=3c3ee9ff1b67a0543a003112cd4bcb13&amp;form_id=06da3f8edbfc08103c4d56f3ce9619f4" target="_blank" rel="noopener noreferrer">here</a>.</p>
</blockquote>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">MERCED</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pinnacles</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>The MERCED (Multi-Environment Research Computer for Exploration and Discovery) Cluster is a 1,872-core, Linux-based high-performance computing system. The MERCED cluster runs with the <a href="https://rockylinux.org/" target="_blank" rel="noopener noreferrer">Rocky (8.10)</a> operating system, and employs the <a href="https://slurm.schedmd.com/" target="_blank" rel="noopener noreferrer">Slurm</a> job scheduler and queueing system to manage job runs. MERCED operates on a <span style="background-color:#3399ff;border-radius:4px;color:#fff;padding:0.2rem 0.5rem;font-weight:bold">Recharge</span> model, meaning users are billed per core-hour of usage. Further details on the recharge process can be found below. To apply for a MERCED account, users must have a Chart of Account (COA) number ready.</p></div></div><p><strong>Facility Statement</strong></p><p>MERCED is a general-purpose computing cluster located in the server facility (see Research Facility below). The cluster consists of a login node, 65 compute nodes, and 15 high memory nodes. Total CPU-core counts is 1872.</p><p><strong>How to cite</strong></p><p>All MERCED users must agree to acknowledge the MERCED Cluster and the supporting UC,Merced Office of Information Technology central funded MERCED in talks, posters, manuscripts, and other forms of dissemination relying on results obtained from time on MERCED. An example acknowledgement section is:</p><p><em>This research [Part of this research] was conducted using MERCED cluster, which is centrally funded by the University of California, Merced, and maintained by the Cyberinfrastructure and Research Technologies (CIRT) team at UC Merced.</em></p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><p>Recharge details</p></summary><div><div class="collapsibleContent_i85q"><div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>MERCED recharge calculations</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Total Cost ($) = # of cores x Duration (wall clock hours) x (cost per core-hour)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul>
<li>A core-hour is a single compute core used for one hour (a core-hour) and 2G of RAM.</li>
<li>Cost per core-hour is $0.01</li>
</ul></div></div><p><strong>why should I be willing to invest in this?</strong></p><ul>
<li>ðŸ”´ The MERCED cluster offers unlimited wall clock time, allowing users to run significantly longer jobs without time constraints.</li>
<li>ðŸŸ¢ The MERCED queue is less crowded, allowing jobs to be picked up with shorter waiting times.</li>
<li>ðŸŸ¡ All recharge funds will go toward cluster maintenance, including network switch replacements and hardware upkeep. The CIRT team will not profit from these contributions.</li>
</ul></div></div></details></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>The NSF-MRI funded Pinnacles cluster located in the server facility (see Research Facility below) is available for all faculty projects at <span style="background-color:#008000;border-radius:4px;color:#fff;padding:0.2rem 0.5rem;font-weight:bold">NO COST</span>! The Pinnacles cluster runs with the <a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux" target="_blank" rel="noopener noreferrer">RedHat</a> operating system, and employs the <a href="https://slurm.schedmd.com/" target="_blank" rel="noopener noreferrer">Slurm</a> job scheduler and queueing system to manage job runs.The Pinnacles cluster is equipped with the latest generation Intel Xeon Gold 6330 CPUs and NVIDIA Tesla A100 v4 40GB HBM2 GPUs. It operates on the RedHat operating system.</p></div></div><p><strong>Facility Statement</strong></p><p>The NSF-MRI grant number #2019144 funded Pinnacles cluster has the following compute node configurations:</p><ul>
<li>40 regular Compute nodes with 2XIntel-28-Core Xeon Gold 6330 2.0GHz - 205W, each with 256GB RAM.</li>
<li>4 High Memory nodes with 2x Intel 28-Core Xeon Gold 6330 2.0GHz CPUs and 1TB RAM for large memory calculations.</li>
<li>8 GPU nodes, and each one of the nodes has 2X NVIDIA Tesla A100 PCIe v4 40GB HBM2 Passive Single GPU.</li>
</ul><p>Pinnacles also has ~92TB NFS Fast Scratch Storage space for accessing large data with low latency and 1.5PB of usable long-term storage.</p><p>Relative proximity and extent of availability: The Pinnacles cluster is managed by the Office of Information Technology at UC Merced and technical support and training opportunities are available. It is available for all faculty projects at no cost. All above nodes are interconnected via HDR InfiniBand w/ RDMA for fast (100Gbits/s) and low latency (sub ms) data transfer.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-cite"><strong>How to cite</strong><a href="#how-to-cite" class="hash-link" aria-label="Direct link to how-to-cite" title="Direct link to how-to-cite">â€‹</a></h3><p>All Pinnacles users must agree to acknowledge the Pinnacles Cluster and the supporting NSF grant (NSF MRI, # 2019144) in talks, posters, manuscripts, and other forms of dissemination relying on results obtained from time on Pinnacles. An example acknowledgement section is:</p><p><em>This research [Part of this research] was conducted using Pinnacles (NSF MRI, # 2019144) at the Cyberinfrastructure and Research Technologies (CIRT) at University of California, Merced.</em></p><p>From time to time the Committee on Research Computing (CoRC) may request a report of publications and presentations authored by Pinnacles users that have included results of calculations on Pinnacles. This information may be used by CoRC in advertising and report documents, future proposals, and/or other materials related to research computing at UC Merced.</p></div></div></div>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><p>Research Facility</p></summary><div><div class="collapsibleContent_i85q"><p>The Research Computing Facility, named the Borg Cube, is a 1,448 square foot pre-manufactured, self-contained, fully serviceable data center-style building comprised of two (2) transportable modular containers, one section for electrical distribution and the other to house research computing machines. The design includes three (3) 60-ton Indirect Evaporative Cooling (IEC) mechanical units to be located outside, adjacent to the structure. The Borg Cube houses twenty (20) 19&quot;-24&quot; racks with 51U per rack, totaling 1020U and provides 400kW of N+1 redundant power capacity at the rack bus, providing the ability to provide individual racks with either redundant or non-redundant power supply units. The design includes two (2) 500kVA PDU&#x27;s each fed from separate sources backed up from dedicated 500kVA UPS which are connected to two (2) 800A rated Starline bus to account for single or dual corded customer connections. An N+1 UPS system will consist of two (2) 500kVA UPS units, each to be equipped with batteries to support its full load for 15 minutes. The mechanical design includes three (3) 60-ton IEC units totaling 633kW of cooling available. This design accounts for an additional unit (N+1), in the case of maintenance or failure of an operating unit. The design technical requirements do not list the requirement for an N+1 mechanical system as this building can be fully supported by two (2) 60-ton units; however, due to the importance of this facility, the third unit exists for operations and maintenance purposes. This approach gives the facility engineers the ability to provide maintenance on a given unit without shutting down the facility and limiting unnecessary down time. A self-contained FM-200 style suppression system along with the required detection devices are provided for fire protection and detection systems. One (1) FM-200 tank is provided for each individual module. A VESDA system is present to detect smoke at its earliest stage and to send a signal to the clean agent suppression panel. Two (2) 1500kva substations based on an N+1 design, are provided to accept separate services. Each substation is provided with a 2000A main breaker, a 1000A breaker for the current facility, and an open space for a future circuit breaker to be used to support a potential future expansion. One of the substations is provided with a 100A breaker for the electrical equipment serving the site loads.</p></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cluster-hardware-configuration">Cluster Hardware Configuration<a href="#cluster-hardware-configuration" class="hash-link" aria-label="Direct link to Cluster Hardware Configuration" title="Direct link to Cluster Hardware Configuration">â€‹</a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Pinnacles Hardware Overview</li><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Merced Hardware</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6" hidden=""><p>Compute nodes: Compute nodes are where actual jobs run. There are three types of compute nodes on Pinnacles.</p><ul>
<li>48 Regular memory (RM) CPU nodes with 256GB RAM</li>
<li>8 Big memory CPU nodes (bigmem) with 1TB RAM</li>
<li>16 GPU Nodes<!-- -->
<ul>
<li>8 GPU nodes with NVIDIA A100 GPUs</li>
<li>8 GPU nodes with NVIDIA L40S GPUs</li>
</ul>
</li>
</ul><table><thead><tr><th style="text-align:left">CPU node</th><th style="text-align:left">RM node</th><th style="text-align:left">bigmem node</th></tr></thead><tbody><tr><td style="text-align:left">Number of nodes</td><td style="text-align:left">40</td><td style="text-align:left">4</td></tr><tr><td style="text-align:left">CPU</td><td style="text-align:left">2 Intel 28 core Xeon Gold 6330</td><td style="text-align:left">2 Intel 28 core Xeon Gold 6330</td></tr><tr><td style="text-align:left">RAM</td><td style="text-align:left">256GB</td><td style="text-align:left">1TB</td></tr><tr><td style="text-align:left">Node-local storage</td><td style="text-align:left">1TB NVMe Data Center Solid State Drive (SSD)</td><td style="text-align:left">1TB NVMe Data Center Solid State Drive (SSD)</td></tr><tr><td style="text-align:left">Network</td><td style="text-align:left">ConnectX-6 VPI adapter card, HDR 100 InfiniBand (100Gb/s) and 100GbE, single-port QSFP56, PCIe3/4 x16 Slot</td><td style="text-align:left">ConnectX-6 VPI adapter card, HDR 100 InfiniBand (100Gb/s) and 100GbE, single-port QSFP56, PCIe3/4 x16 Slot</td></tr></tbody></table><table><thead><tr><th style="text-align:left"><code>cenvalarc</code> CPU node</th><th style="text-align:left"><code>cenvalarc.compute</code> - CPU Node</th><th style="text-align:left"><code>cenvalarc.bigmem</code> - bigmem node</th></tr></thead><tbody><tr><td style="text-align:left">Number of nodes</td><td style="text-align:left">8</td><td style="text-align:left">4</td></tr><tr><td style="text-align:left">CPU</td><td style="text-align:left">2x Intel 32-Core Xeon Gold 6530 2.1GHz - 270W</td><td style="text-align:left">2x Intel 32-Core Xeon Gold 6530 2.1GHz - 270W</td></tr><tr><td style="text-align:left">RAM</td><td style="text-align:left">256GB</td><td style="text-align:left">1TB</td></tr><tr><td style="text-align:left">Node-local storage</td><td style="text-align:left">1TB M.2 NVMe Data Center Solid State Drive (110mm)</td><td style="text-align:left">1TB M.2 NVMe Data Center Solid State Drive (110mm)</td></tr><tr><td style="text-align:left">Network</td><td style="text-align:left">ConnectX-6 VPI adapter card, HDR-100 IB (100Gb/s) and 100GbE, single-port QSFP56, PCIe3/4 x16 Slot</td><td style="text-align:left">ConnectX-6 VPI adapter card, HDR-100 IB (100Gb/s) and 100GbE, single-port QSFP56, PCIe3/4 x16 Slot</td></tr></tbody></table><table><thead><tr><th style="text-align:left"><code>gpu</code> GPU node</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">Number</td><td style="text-align:left">8</td></tr><tr><td style="text-align:left">GPU per node</td><td style="text-align:left">2x NVIDIA Tesla A100 PCIe v4 40GB HBM2 Passive Single GPU</td></tr><tr><td style="text-align:left">CPU</td><td style="text-align:left">2x Intel 28-Core Xeon Gold 6330</td></tr><tr><td style="text-align:left">RAM</td><td style="text-align:left">256GB</td></tr><tr><td style="text-align:left">Node-local storage</td><td style="text-align:left">1TB M.2 NVMe Data Center Solid State Drive (110mm)</td></tr><tr><td style="text-align:left">Network</td><td style="text-align:left">ConnectX-6 VPI adapter card, HDR-100 IB (100Gb/s) and 100GbE, single-port QSFP56, PCIe3/4 x16 Slot</td></tr></tbody></table><table><thead><tr><th style="text-align:left"><code>cenvalarc.gpu</code> GPU node</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">Number</td><td style="text-align:left">8</td></tr><tr><td style="text-align:left">GPU per node</td><td style="text-align:left">2x NVIDIA L40S GPUs per node (48GB GDDR6 Passive Dual Slot GPU)</td></tr><tr><td style="text-align:left">CPU</td><td style="text-align:left">2x Intel 32-Core Xeon Gold 6530 2.1GHz - 270W</td></tr><tr><td style="text-align:left">RAM</td><td style="text-align:left">256GB</td></tr><tr><td style="text-align:left">Node-local storage</td><td style="text-align:left">1TB M.2 NVMe Data Center Solid State Drive (110mm)</td></tr><tr><td style="text-align:left">Network</td><td style="text-align:left">ConnectX-6 VPI adapter card, HDR-100 IB (100Gb/s) and 100GbE, single-port QSFP56, PCIe3/4 x16 Slot</td></tr></tbody></table></div><div role="tabpanel" class="tabItem_Ymn6"><p>MERCED hosts 66 CPU compute nodes including 25 high memory nodes. Please be aware that
the nodes among MERCED cluster are multigenerational, meaning that the CPU
processors from different nodes are having different features, the table shows below
listed detailed node information. Users may experience relative big
performance variations when running the same jobs on different nodes.</p><p>The table below listed all MERCED cluster CPU compute nodes features, and their
processors generations.</p><table><thead><tr><th>Nodes</th><th>feature</th><th>RAM</th><th>Total cores per nodes</th><th>InfiniBand (IB)</th></tr></thead><tbody><tr><td>33-43</td><td>Broadwell,avx2,E5-2650_v4,local scratch 932GB</td><td>128GB</td><td>24</td><td>yes</td></tr><tr><td>44</td><td>Broadwell,avx2,E5-2650_v4,local scratch 932GB</td><td>112GB</td><td>24</td><td>yes</td></tr><tr><td>45-60</td><td>Broadwell,avx2,E5-2650_v4,local scratch 932GB</td><td>257GB</td><td>24</td><td>yes</td></tr><tr><td>61-72</td><td>Broadwell,avx2,E5-2650_v4,local scratch 447GB</td><td>257GB</td><td>24</td><td>yes</td></tr><tr><td>73-76, 79-88</td><td>Broadwell,avx2,E5-2650_v4,local scratch 932GB</td><td>128GB</td><td>24</td><td>yes</td></tr><tr><td>77</td><td>Broadwell,avx2,E5-2650_v4,no local scratch</td><td>128GB</td><td>24</td><td>yes</td></tr><tr><td>89-104</td><td>Skylake,sse4.2,avx,avx2,avx512,Gold_6130, no local scratch</td><td>191GB</td><td>32</td><td>yes</td></tr><tr><td>105-114</td><td>cascadelake,sse4.2,avx,avx2,avx512,Gold_6230, no local scratch</td><td>191GB</td><td>40</td><td>yes</td></tr></tbody></table></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-request-an-account">How to Request an Account<a href="#how-to-request-an-account" class="hash-link" aria-label="Direct link to How to Request an Account" title="Direct link to How to Request an Account">â€‹</a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Pinnacles Account Process</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">MERCED Account Process</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><p>UC Merced Faculty Principal Investigators (PIs) can request access to Pinnacles cluster. All student user accounts on Pinnacles cluster must associate with UC Merced PIs.</p><p>UC Merced Principal Investigators (PIs) or other researchers request Pinnacles account <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=643ea9ff1b67a0543a003112cd4bcba3&amp;form_id=280d8bb04f72f6006137d0af0310c7b0" target="_blank" rel="noopener noreferrer">here</a>.</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click Here to View a Visual Guide for Creating an Account for Pinnacles</summary><div><div class="collapsibleContent_i85q"><p><strong>Requesting Access to Pinnacles Process.</strong></p><ol>
<li>UC Merced Principal Investigators (PIs) or other researchers request Pinnacles account <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=643ea9ff1b67a0543a003112cd4bcba3&amp;form_id=280d8bb04f72f6006137d0af0310c7b0" target="_blank" rel="noopener noreferrer">here</a>.<!-- -->
<ol>
<li>For new account group project applications, PIs please also make sure to complete the export control <a href="https://ucmerced.app.box.com/s/zvptfc8adbdzt4xs8kcj73lyretyn692" target="_blank" rel="noopener noreferrer">form</a>, if the PI has not done one before.</li>
<li>Once the form is completed, please attach the form to the request ticket scene in the following steps.</li>
</ol>
</li>
<li>Click <code>Request Service</code>
<img decoding="async" loading="lazy" alt="Image of Request Service" src="/hpc_doc_new/assets/images/RequestAccountpt2-e2d56d1345186507dfbd66e3a250c28f.png" title="Request Account Button" width="3018" height="1524" class="img_ev3q"></li>
<li>Begin to populate all the required information.<!-- -->
<ol>
<li>At the question regardig PI Status. Typically only Professors are PIs, their students and post-docs would select <code>No</code> at this question.
<img decoding="async" loading="lazy" alt="Image of PI Selection" src="/hpc_doc_new/assets/images/RequestAccountpt2PI-f2b3a311f06f6c4d885b0312e79abfdc.png" title="PI Selection" width="3018" height="1524" class="img_ev3q"></li>
</ol>
</li>
<li>For selecting the system, from the drop-down, click <code>pinnacles.ucmerced.edu (Free Cluster)</code>
<img decoding="async" loading="lazy" alt="Image of Pinnacles Selection" src="/hpc_doc_new/assets/images/RequestAccountPinnacles-1126df57c8a09d1d9e4c6ed56dc57d8d.png" title="Selecting Pinnacles" width="3018" height="1524" class="img_ev3q"></li>
<li>Add any other additonal comments or information, you believe will be helpful for the requesting an account process.</li>
<li>Click <code>Request Service</code>
<img decoding="async" loading="lazy" alt="Submitting Ticket" src="/hpc_doc_new/assets/images/RequestAccount3-0f9b0a013e1f12486659ca71502f0bf5.png" title="Submitting Ticket" width="3018" height="1524" class="img_ev3q"></li>
</ol></div></div></details></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><p>UC Merced Faculty Principal Investigators (PIs) can request access to MERCED cluster. All student user accounts on MERCED cluster must associate with UC Merced PIs.   UC Merced Principal Investigators (PIs) or other researchers request MERCED account <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=643ea9ff1b67a0543a003112cd4bcba3&amp;form_id=280d8bb04f72f6006137d0af0310c7b0" target="_blank" rel="noopener noreferrer">here</a>.</p><div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>MERCED is a recharge cluster, and will require all accounts to have an associated COA Number submitted at time of account request.</p></div></div><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click Here to View a Visual Guide for Creating an Account for MERCED</summary><div><div class="collapsibleContent_i85q"><p><strong>Requesting Access to MERCED Process.</strong></p><ol>
<li>UC Merced Principal Investigators (PIs) or other researchers request MERCED account <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=643ea9ff1b67a0543a003112cd4bcba3&amp;form_id=280d8bb04f72f6006137d0af0310c7b0" target="_blank" rel="noopener noreferrer">here</a>.<!-- -->
<ol>
<li>For new account group project applications, PIs please also make sure to complete the export control <a href="https://ucmerced.app.box.com/s/zvptfc8adbdzt4xs8kcj73lyretyn692" target="_blank" rel="noopener noreferrer">form</a>, if the PI has not done one before.</li>
<li>Once the form is completed, please attach the form to the request ticket scene in the following steps.</li>
</ol>
</li>
<li>Click <code>Request Service</code>
<img decoding="async" loading="lazy" alt="Image of Request Service" src="/hpc_doc_new/assets/images/RequestAccountpt2-e2d56d1345186507dfbd66e3a250c28f.png" title="Request Account Button" width="3018" height="1524" class="img_ev3q"></li>
<li>Begin to populate all the required information.<!-- -->
<ol>
<li>At the question regardig PI Status. Typically only Professors are PIs, their students and post-docs would select <code>No</code> at this question.
<img decoding="async" loading="lazy" alt="Image of PI Selection" src="/hpc_doc_new/assets/images/RequestAccountpt2PI-f2b3a311f06f6c4d885b0312e79abfdc.png" title="PI Selection" width="3018" height="1524" class="img_ev3q"></li>
</ol>
</li>
<li>For selecting the system, from the drop-down, click <code>merced.ucmerced.edu (Recharge Cluster)</code>
<img decoding="async" loading="lazy" alt="Image of MERCED Selection" src="/hpc_doc_new/assets/images/RequestAccountMERCED-ed684ecdade6f4ef94c473b227c67b31.png" title="Selecting MERCED" width="3018" height="1524" class="img_ev3q"></li>
<li>Because MERCED is a Recharge Cluster, please attach a valid COA number. Without a COA number, the account request will be denied.
<img decoding="async" loading="lazy" alt="Image of COA Input" src="/hpc_doc_new/assets/images/RequestAccountMERCEDCOA-e2c61a1cd8ef87343aa25abc2cb485d5.png" title="COA Input" width="3018" height="1524" class="img_ev3q"></li>
<li>Add any other additonal comments or information, you believe will be helpful for the requesting an account process.</li>
<li>Click <code>Request Service</code>
<img decoding="async" loading="lazy" alt="Submitting Ticket" src="/hpc_doc_new/assets/images/RequestAccount3-0f9b0a013e1f12486659ca71502f0bf5.png" title="Submitting Ticket" width="3018" height="1524" class="img_ev3q"></li>
</ol></div></div></details></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="centralized-login">Centralized login<a href="#centralized-login" class="hash-link" aria-label="Direct link to Centralized login" title="Direct link to Centralized login">â€‹</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="login-nodes">Login nodes<a href="#login-nodes" class="hash-link" aria-label="Direct link to Login nodes" title="Direct link to Login nodes">â€‹</a></h3>
<p>The standard method for connecting to a remote machine is through Secure Shell (<code>ssh</code>) commands. Starting from 02/01/2023, we will implement a <span style="background-color:#0F2D52;border-radius:4px;color:#fff;padding:0.2rem 0.5rem;font-weight:bold">centralized login</span> system. This means that once a user logs into one of the login nodes, they will be able to access both the MERCED and Pinnacles clusters. Users applying for a Pinnacles account can begin the application process <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=643ea9ff1b67a0543a003112cd4bcba3&amp;form_id=280d8bb04f72f6006137d0af0310c7b0" target="_blank" rel="noopener noreferrer">here</a>, and Pinnacles is <strong>FREE</strong> to use within the campus. However, to access the MERCED cluster, users must provide a <strong>COA</strong> account number and enter the number during the MERCED account application process.</p>
<p>Currently, we have three login nodes, and users can expect to be connected to either <code>rclogin01</code>, <code>rclogin02</code>, or <code>rclogin03</code>. <strong>Do not run computationally intensive processes on the login nodes.</strong> These nodes are appropriate for tasks such as file preparation/editing, compiling, simple analyses, and other low-computation activities. For more resource-intensive work, submit jobs to the cluster using the available queue system. Additionally, users can connect to a remote machine using an X-terminal (XQuarz or X11) forwarding (see example command below) to run graphics-based programs like gnuplot, gimp, etc.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="connect-to-the-clusters">Connect to the clusters<a href="#connect-to-the-clusters" class="hash-link" aria-label="Direct link to Connect to the clusters" title="Direct link to Connect to the clusters">â€‹</a></h3>
<p>On Mac and Linux you can use the built-in terminal application; on Windows you can use <a href="https://mobaxterm.mobatek.net/" target="_blank" rel="noopener noreferrer">MobaXterm</a> to open a terminal, and type the following command, but replace <code>&lt;username&gt;</code> to your UCMID.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ssh &lt;username&gt;@login.rc.ucmerced.edu</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="x11-forwarding">X11 forwarding<a href="#x11-forwarding" class="hash-link" aria-label="Direct link to X11 forwarding" title="Direct link to X11 forwarding">â€‹</a></h3>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p><strong>Mac OS:</strong></p><p>Prerequisite: Install <a href="https://www.xquartz.org/" target="_blank" rel="noopener noreferrer">XQuartz</a>
Then open XQuartz through <code>open -a XQuartz</code> in the terminal or other CLI. Then type the following command.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ssh -X &lt;username&gt;@login.rc.ucmerced.edu</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul>
<li><code>-X</code>: Enables X11 forwarding.</li>
</ul></div></div>
<p><strong>Windows users</strong></p>
<p>MobaXterm includes an integrated X11 server, so no additional installation of X11 software is needed.</p>
<ul>
<li>Start a New SSH Session with X11 Forwarding<!-- -->
<ul>
<li>In the top-left corner, click on the &quot;Session&quot; button.</li>
<li>Choose &quot;SSH&quot; from the available options</li>
</ul>
</li>
<li>Configure the SSH Session<!-- -->
<ul>
<li>In the Remote Host field, enter the address of the remote server (e.g., remote.server.com)</li>
<li>Ensure the &quot;Specify username&quot; box is checked, then enter your username for the remote server</li>
<li>Check the box that says &quot;X11-forwarding&quot;. This option enables X11 forwarding for your session</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="file-systems-and-storage">File systems and storage<a href="#file-systems-and-storage" class="hash-link" aria-label="Direct link to File systems and storage" title="Direct link to File systems and storage">â€‹</a></h2>
<p>There are 2 folders (<code>data</code> and <code>scratch</code>) locate in <code>HOME</code> that users will start with.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>MERCED and Pinnacles have now been merged into a centralized system, allowing them to share the same file systems. We have also increased the quota for the <code>data</code>, <code>scratch</code>, and <code>HOME</code> directories. Please note that there is a 7-day grace period once the soft quota limit is reached.</p></div></div>
<table><thead><tr><th>Folder</th><th>soft quota</th><th>hard quota</th></tr></thead><tbody><tr><td><code>HOME</code></td><td>70G</td><td>75G</td></tr><tr><td><code>data</code></td><td>500G</td><td>512G</td></tr><tr><td><code>scratch</code></td><td>500G</td><td>512G</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="queue-information">Queue Information<a href="#queue-information" class="hash-link" aria-label="Direct link to Queue Information" title="Direct link to Queue Information">â€‹</a></h2>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Pinnacles Queue Information</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">MERCED Queue Information</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><p>Pinnacles Cluster is the default cluster that is free and accessible to all users and has 6 public queues.</p><table><thead><tr><th>Public Queues(Available to all users)</th><th>Max Wall Time</th><th>Default Time</th><th>Max Nodes per Job</th><th>Max # of jobs that can be submitted</th></tr></thead><tbody><tr><td>^test</td><td>1 hour</td><td>5 min.</td><td>2 nodes</td><td>1</td></tr><tr><td>bigmem</td><td>3 days</td><td>1 hrs</td><td>2 nodes</td><td>2</td></tr><tr><td>gpu</td><td>3 days</td><td>1 hrs</td><td>2 nodes</td><td>4</td></tr><tr><td>*short</td><td>6 hours</td><td>1 hrs</td><td>4 nodes</td><td>12</td></tr><tr><td>medium</td><td>1 day</td><td>6 hrs</td><td>4 nodes</td><td>6</td></tr><tr><td>long</td><td>3 days</td><td>1 day</td><td>4 nodes</td><td>3</td></tr><tr><td>cenvalarc.compute</td><td>3 day</td><td>1 day</td><td>4 nodes</td><td>3</td></tr><tr><td>cenvalarc.bigmem</td><td>3 day</td><td>1 day</td><td>2 nodes</td><td>2</td></tr><tr><td>cenvalarc.gpu</td><td>3 day</td><td>1 day</td><td>2 nodes</td><td>4</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p><code>short</code> queue is the default queue for all jobs submitted without specifiying which queue job must run on</p><p>^test queue has access to all node types use constraints to test on specific types. Ex:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"> #SBATCH --constraint=gpu,bigmem</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Access to GPUs also requires</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">#SBATCH --gres=gpu:X</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><p>MERCED is the <strong>Recharge</strong> Cluster.</p><table><thead><tr><th>Public Queues(Available to all users)</th><th>Max Wall Time</th><th>Default Time</th><th>Max Nodes per Job</th><th>Max # of jobs that can be submitted</th></tr></thead><tbody><tr><td>bigmem</td><td>5 days</td><td>1 hr</td><td>2 nodes</td><td>6</td></tr><tr><td>test^</td><td>1 hour</td><td>5 min.</td><td>2 nodes</td><td>1</td></tr><tr><td>*compute</td><td>5 days</td><td>1 hr</td><td>2 nodes</td><td>6</td></tr></tbody></table><div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p><code>#SBATCH -M merced </code> must always be used to submit a job to MERCED cluster</p><p>^ <code>test</code> queue has access to all node types use constraints to test on specific types.</p><p><code>compute</code> queue is the default queue for all jobs submitted</p></div></div></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="global-modules-on-pinnacles-and-merced">Global Modules on Pinnacles and MERCED<a href="#global-modules-on-pinnacles-and-merced" class="hash-link" aria-label="Direct link to Global Modules on Pinnacles and MERCED" title="Direct link to Global Modules on Pinnacles and MERCED">â€‹</a></h2>
<p>Pinnacles and MERCED already come with a collection of global modules or softwares that do not need to be individually installed by the user. The modele system allows for the <em>loading</em> and <em>unloading</em> of a specific module. Users will make use of <code>avail</code>, <code>load</code>, <code>list</code>, <code>unload</code>, and <code>swap</code>. A table describing each of these Modules options is given below.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>A complete guide to using modules can be found via <code>man module</code>.</p></div></div>
<table><thead><tr><th style="text-align:left">Command</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>module avail</code></td><td style="text-align:left">This command lists all available modules</td></tr><tr><td style="text-align:left"><code>module load &lt;mod_name&gt;</code></td><td style="text-align:left">This command loads the environment corresponding to &lt;mod_name&gt;</td></tr><tr><td style="text-align:left"><code>module list</code></td><td style="text-align:left">This command provides a list of all modules currently loaded into the user environment</td></tr><tr><td style="text-align:left"><code>module unload &lt;mod_name&gt;</code></td><td style="text-align:left">This command unloads  the environment corresponding to &lt;mod_name&gt;</td></tr><tr><td style="text-align:left"><code>module swap &lt;mod_1&gt; &lt;mod_2&gt;</code></td><td style="text-align:left">This command unloads the environment corresponding to &lt;mod_1&gt; and loads to &lt;mod_2&gt;</td></tr></tbody></table>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Pinnacles and MERCED Global Modules</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">MERCED Only Modules</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary> Click Here to Expand to View the List. </summary><div><div class="collapsibleContent_i85q"><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">   admin/0.0.1                            gaussian/gdv-20170407-i10+             mpfr/4.2.0                               r-biobase/2.50.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">amber/20-devel                         gaussian/gdv-20210302-j15       (D)    mpich/3.4.2-gcc-8.4.1                    r-ctc/1.64.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">amber/20                        (D)    gcc/8.5.0                              mpich/3.4.2-intel-2021.4.0               r-deseq2/1.30.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">anaconda3/2021.05                      gcc/11.2.0                             mpich/3.4.2-nvidiahpc-21.9-0      (D)    r-edger/3.32.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">anaconda3/2023.09-0             (D)    gcc/12.2.0                      (D)    multiqc/1.7                              r-fastcluster/1.1.25</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">angsd/0.940                            git/2.37.0                             multiwfn/3.8                             r-glimma/2.0.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">apbs/3.4.1                             glpk/4.65                              mvapich2/2.3.6-gcc-8.4.1                 r-goplot/1.0.2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">awscli/1.16.308                        gmp/6.2.1                              mvapich2/2.3.6-intel-2021.4.0     (D)    r-goseq/1.42.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bamtools/2.5.1                         gnuplot/5.4.2                          ncbi-blast+/2.12.0                       r-gplots/3.1.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bamutil/1.0.15                         grace/5.1.25                           netlib-lapack/3.9.1                      r-qvalue/2.22.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bbmap/39.06                            gromacs/2021.3                         netlib-xblas/1.0.248                     r-rots/1.18.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bcftools/1.12                          gromacs/2022.3                         nvidiahpc/21.9-0                         r-sm/2.2-5.6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bcftools/1.14                   (D)    gromacs/2023.1                  (D)    octopus/13.0                             r-tidyverse/1.3.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bcl2fastq2/2.20.0.422                  gsl/2.7                                octopus/14.1                      (D)    r/4.1.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">beast/1.10.4                           gurobi/9.5.0                           onnx/1.10.1                              r/4.2.2                          (D)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">beast2/2.6.4                           hdf5/1.10.7-intel-2021.4.0             openbabel/3.0.0                          raxml-ng/1.2.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bedtools2/2.30.0                       hdf5/1.14.1-2                   (D)    openblas/0.3.18                          rclone/1.59.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">berkeleygw/3.0.1-intel-mvapich2        ibamr/0.8.0-testing                    openblas/0.3.21                   (D)    repeatmodeler/1.0.11</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">berkeleygw/3.0.1-intel-2021.4.0        ibamr/0.12.0-debug                     opencarp/8.1                             rsem/1.3.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">berkeleygw/3.0.1                       ibamr/0.12.0-opt                       openjdk/1.8.0_265-b01                    salmon/1.4.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">berkeleygw/4.0-mvapich2-oneapi  (D)    ibamr/0.13.0-debug                     openjdk/11.0.20                          samtools/1.13</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">blast-plus/2.12.0                      ibamr/0.13.0-opt                (D)    openjdk/17.0.5_8                  (D)    scalapack/2.1.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bowtie/1.3.0                           intel/oneapi                           openmpi/3.1.3-gcc                        schrodinger/2022-1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bowtie2/2.4.2                          interproscan/5.55-88.0                 openmpi/3.1.6-gcc-8.4.1                  schrodinger/2022-3               (D)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">braker/2.1.6                           ior/3.3.0                              openmpi/3.1.6-intel-2021.4.0             sickle/1.33</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">butterflypack/2.0.0                    iq-tree/2.1.3                          openmpi/3.1.6-nvidiahpc-21.9-0           singularity/3.8.3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bwa-mem2/2.2.1                         jellyfish/2.2.7                        openmpi/4.0-merced-test                  smalt/0.7.6</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bwa/0.7.17                             julia/1.7.3                            openmpi/4.0.6-gcc-8.4.1                  sombrero/2021-08-16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">casacore/3.4.0                         julia/10.1.1                    (D)    openmpi/4.0.6-intel-2021.4.0             spiral/8.2.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cgal/5.0.3                             kallisto/0.46.2                        openmpi/4.0.6-nvidiahpc-21.9-0           srilm/1.7.3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmake/3.21.4                           lammps/20210310+kokkos+cuda            openmpi/4.1.1-gcc-8.4.1                  stacks/2.53</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cmaq/5.3.1                             lammps/20210310+user-omp+kokkos        openmpi/4.1.1-intel-2021.4.0             star/2.7.11b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">collier/1.2.5                          lammps/20210310                        openmpi/4.1.4-gcc-12.2.0+cuda            stata/17</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cuda/10.2.89                           lammps/20220107+ml-quip                openmpi/4.1.4-gcc-12.2.0          (D)    stata/18                         (D)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cuda/11.0.3                            lammps/20220107                        orca/5.0.1                               stringtie/2.2.3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cuda/11.4.0                            lammps/20230208                 (D)    orthofinder/2.5.2                        subversion/1.14.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cuda/11.5.0                            latte/1.2.2                            perl-db-file/1.840                       suite-sparse/5.13.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cuda/11.8.0                            lftp/4.9.2                             perl-uri/1.72                            tcl/8.5.19-gcc-8.5.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cuda/12.3.0                     (D)    libraries                              perl/5.34.0                              terachem/1.95</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dakota/6.12                            libtirpc/1.1.4                         phyluce/1.6.7                            tk/8.5.19-gcc-8.5.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dalton/2020.0                          libxc/5.2.3-gcc-12.2.0                 picard/2.26.2                            toolchain/scientificstack-11.2.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">elpa/2021.11.001                       likwid/5.2.2+cuda                      pigz/2.7                                 transdecoder/5.5.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">emacs/27.2                             likwid/5.2.2                    (D)    plink/1.90-beta-7.1                      trimmomatic/0.39</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">express/1.5.2                          localcolabfold/1.5.1                   protobuf/3.18.0                          trinity/2.12.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fastqc/0.11.9                          mathematica/12.3.1                     py-numpy/1.21.4                          trinity/2.15.1                   (D)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ffmpeg/4.3.2                           mathematica/14.0.0              (D)    python/3.8.12                            user-modules</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fftw/3.3.10-gcc-8.5.0                  matlab/r2021b                          python/3.11.0                     (D)    vcftools/0.1.14</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fftw/3.3.10-intel-2021.4.0      (D)    matlab/r2023a                          quantum-espresso/6.7-intel-test          vmd/1.9.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gate/9.0                               matlab/r2024a                   (D)    quantum-espresso/7.1                     vmd/1.9.3                        (D)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gatk/4.2.6                             metis/5.1.0                            quantum-espresso/7.2-gcc-openblas (D)    wannier90/3.1.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gaussian/g09-d01                       minimap2/2.14                          r-ape/5.4-1                              xcrysden/1.6.2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">gaussian/g16-b01                       molden/6.7                             r-argparse/2.0.3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Where:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">D:  Default Module</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></details></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">   berkeleygw/3.0.1-intel-mvapich2    berkeleygw/4.0-mvapich2-oneapi (D)    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   bwa-mem2/2.2.1    openmpi/4.0-merced-test   </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  user-modules</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Where:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  D:  Default Module</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="checking-disk-quota-and-usage">Checking disk quota and usage<a href="#checking-disk-quota-and-usage" class="hash-link" aria-label="Direct link to Checking disk quota and usage" title="Direct link to Checking disk quota and usage">â€‹</a></h3>
<p>To look at your current usage amounts of <code>HOME</code>, <code>data</code> or <code>scratch</code> use the following command</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">quota -vs </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>This will output, in sections, the filesystem, current space usage, quota, hard limit, and other relevant information in a more readable format.</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>To convert the outputted megabytes to gigabytes = space(MB) divided by 1024</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="checking-the-size-of-directories-and-content">Checking the size of directories and content<a href="#checking-the-size-of-directories-and-content" class="hash-link" aria-label="Direct link to Checking the size of directories and content" title="Direct link to Checking the size of directories and content">â€‹</a></h3>
<p>To chek the size of the current directory or any directories in it use the <code>du</code> command.</p>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p><code>du</code> command alone will output all directories, hidden as well, in real time so it will take a few momments to finish. It is recommended to execute the command with some of the following options to make the process more clear and consice.</p></div></div>
<table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>-h</code></td><td>Displays storage values in human-readable format (e.g., KB, MB, GB)</td></tr><tr><td><code>-s</code></td><td>Summarizes the size of the whole current directory</td></tr><tr><td><code>-sh</code></td><td>Shows the size of the specified sub-directory</td></tr><tr><td><code>--max-depth=N</code></td><td>Shows the sub-directories up to depth N, where N is a number representing the max depth</td></tr><tr><td><code>--all</code></td><td>Writes counts for all files, not just directories</td></tr><tr><td><code>--help</code></td><td>Displays all other options for the <code>du</code> command</td></tr></tbody></table>
<p>Example usage of <code>du</code> command:</p>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">example</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">output</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">du -h -s &lt;directory name&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">35G     &lt;directory name&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></div>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Users who submit jobs to MERCED and use the unified storage are expecting slower network communications.</p><ul>
<li>Home - Shared over 10G network from Pinnacles to Merced, connected over IB on pinnacles.</li>
<li>Data - Shared over 10G network from Pinnacles to Merced, connected over IB on pinnacles.</li>
<li>Scratch - Shared over 10G network from Pinnacles to Merced, connected over IB on pinnacles</li>
</ul><p>The <code>scratch</code> folder is purged periodically when the overall system storage reaches 85% of capacity or higher. Please back-up your data to somewhere safe frequently.</p><p>Please avoid writing files directly to <code>/tmp</code> on the head node, as this can fill up disk space and cause issues for all users. Instead, use your personal scratch directory for temporary files. Some programs may default to using <code>/tmp</code>, so ensure that the appropriate scratch directory is properly configured for your code.</p></div></div>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>Disclaimer: Users are responsible for backing up all data stored on the clusters and are fully accountable for its availability. CIRT is not liable for any data loss in the event of accidents</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="borgstore-beegfs">Borgstore (BeeGFS)<a href="#borgstore-beegfs" class="hash-link" aria-label="Direct link to Borgstore (BeeGFS)" title="Direct link to Borgstore (BeeGFS)">â€‹</a></h2>
<p>Borgstore is UC Merced&#x27;s on campus data-storage center and is available for research PI to purchase. Borgstore is used for store active,research-related data and has the use capacity of 480 terrabytes.</p>
<p>Borgstore is also made up of a metadata server with a <code>1U dual Xeon Skylake SP</code>, <code>8x 2.5&quot; Hotswap</code>, <code>24 DIMM Sockets</code>, <code>IPMI</code>, <code>dual 10GbE-T base system</code> and <code>8x 3.2TB Endurance Data Center PCIe NVMe 2.5</code>.Solid State Drive metadata storage</p>
<p>Borgstore is now accessible on all nodes (MERCED and Pinnacles) with Infiniband (IB), all of the partitions contain a mixture of nodes of InfiniBand and non-InfiniBand nodes. If users want to submit jobs while at the Borgstore folder, the slurm option of #SBATCH --constraint=ib should be added to the job script. Example job script can be found here. More information pertaining to Borgstore can be found here</p>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Borgstore does not do data backups. Users are responsible for ensuring they have backups of thier data and work.</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="request-borgstore">Request Borgstore<a href="#request-borgstore" class="hash-link" aria-label="Direct link to Request Borgstore" title="Direct link to Request Borgstore">â€‹</a></h3>
<p>Borgstore is a purchasable service that can be requested <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=3c3ee9ff1b67a0543a003112cd4bcb13&amp;form_id=06da3f8edbfc08103c4d56f3ce9619f4" target="_blank" rel="noopener noreferrer">here</a>. Cost for active data storage is shown below. More information can be found <a href="https://it.ucmerced.edu/Research-Computing-Services" target="_blank" rel="noopener noreferrer">here</a>.</p>
<table><thead><tr><th>Startup funds</th><th>Non-startup funds</th></tr></thead><tbody><tr><td>$0.05/GB/year</td><td>$0.06/GB/year</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accessing-data-from-borgstore-for-a-job">Accessing Data from Borgstore for a job<a href="#accessing-data-from-borgstore-for-a-job" class="hash-link" aria-label="Direct link to Accessing Data from Borgstore for a job" title="Direct link to Accessing Data from Borgstore for a job">â€‹</a></h3>
<p>To access data that is located in Borgstore the user must be in the Borgstore folder/directory and must the submit job to the scheduler from this location. If users want to submit jobs while at the Borgstore folder, the slurm option of <code>#SBATCH --constraint=ib</code> should be added to the job script.</p>
<p>To run an interactive job, the job must also be ran from the borgstore directory.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="common-questions-about-borgstore">Common questions about Borgstore<a href="#common-questions-about-borgstore" class="hash-link" aria-label="Direct link to Common questions about Borgstore" title="Direct link to Common questions about Borgstore">â€‹</a></h3>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>How to check my current occupied space on Borgstore? </summary><div><div class="collapsibleContent_i85q"><p>Users can not check a detailed-breakdown of a group&#x27;s users and their respective useage of the Borgstore storage. However, users can check thier Groups storage usage/total using the command <code>beegfs-ctl --getquota --gid</code>. And users can check their ID number and Group ID via the command <code>id</code></p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>When should I use Borgstore? </summary><div><div class="collapsibleContent_i85q"><p>Borgstore is a great for users who must readily access exceedingly large quantities of data that can not fit onto our given data quotes in the shared workspace of <code>Home</code>,<code>Data</code> and <code>Scratch</code>. Borgstore is also useful when the output(s) are huge and can quickly take up all the space in the <code>Data</code> folder</p></div></div></details>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>I am having trouble accessing the Borgstore and the direcotries in the Command-Line Interface is blanking red?</summary><div><div class="collapsibleContent_i85q"><p>If the path displayed is flashing red and user is having issues directly accessing the Borgstore then the soft-links leading to the Borgstore are broken or outdated. It is best to open a ticket to get further assitance. Click <a href="https://ucmerced.service-now.com/servicehub?id=public_kb_article&amp;sys_id=3c3ee9ff1b67a0543a003112cd4bcb13&amp;form_id=06da3f8edbfc08103c4d56f3ce9619f4" target="_blank" rel="noopener noreferrer">here</a> to open a ticket.</p></div></div></details></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/amirayuyue/hpc_doc_new/docs/hpcdocs/HPC-clusters/campus-clusters.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/policies"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">University Policies and HPC Guidelines</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/condo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Condo Model and Cluster Structure</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#hpc-clusters" class="table-of-contents__link toc-highlight">HPC Clusters</a><ul><li><a href="#how-to-cite" class="table-of-contents__link toc-highlight"><strong>How to cite</strong></a></li></ul></li><li><a href="#cluster-hardware-configuration" class="table-of-contents__link toc-highlight">Cluster Hardware Configuration</a></li><li><a href="#how-to-request-an-account" class="table-of-contents__link toc-highlight">How to Request an Account</a></li><li><a href="#centralized-login" class="table-of-contents__link toc-highlight">Centralized login</a><ul><li><a href="#login-nodes" class="table-of-contents__link toc-highlight">Login nodes</a></li><li><a href="#connect-to-the-clusters" class="table-of-contents__link toc-highlight">Connect to the clusters</a></li><li><a href="#x11-forwarding" class="table-of-contents__link toc-highlight">X11 forwarding</a></li></ul></li><li><a href="#file-systems-and-storage" class="table-of-contents__link toc-highlight">File systems and storage</a></li><li><a href="#queue-information" class="table-of-contents__link toc-highlight">Queue Information</a></li><li><a href="#global-modules-on-pinnacles-and-merced" class="table-of-contents__link toc-highlight">Global Modules on Pinnacles and MERCED</a><ul><li><a href="#checking-disk-quota-and-usage" class="table-of-contents__link toc-highlight">Checking disk quota and usage</a></li><li><a href="#checking-the-size-of-directories-and-content" class="table-of-contents__link toc-highlight">Checking the size of directories and content</a></li></ul></li><li><a href="#borgstore-beegfs" class="table-of-contents__link toc-highlight">Borgstore (BeeGFS)</a><ul><li><a href="#request-borgstore" class="table-of-contents__link toc-highlight">Request Borgstore</a></li><li><a href="#accessing-data-from-borgstore-for-a-job" class="table-of-contents__link toc-highlight">Accessing Data from Borgstore for a job</a></li><li><a href="#common-questions-about-borgstore" class="table-of-contents__link toc-highlight">Common questions about Borgstore</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hpc_doc_new/docs/hpcdocs/hpc-tutorials/intro-hpc">HPC Tutorial</a></li><li class="footer__item"><a class="footer__link-item" href="/hpc_doc_new/docs/hpcdocs/HPC-clusters/campus-clusters/#hpc-clusters">HPC Clusters</a></li><li class="footer__item"><a class="footer__link-item" href="/hpc_doc_new/docs/jupyter/jupyterhub">JupyterHub</a></li></ul></div><div class="col footer__col"><div class="footer__title">Support</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ucmerced.service-now.com/servicehub" target="_blank" rel="noopener noreferrer" class="footer__link-item">ServiceNow Hub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ucmerced.zoom.us/j/89487493900" target="_blank" rel="noopener noreferrer" class="footer__link-item">Zoom Office Hours (Fridays 11:30pm-1pm PT<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://it.ucmerced.edu/CIRT" target="_blank" rel="noopener noreferrer" class="footer__link-item">CIRT UC Merced Home Page<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ucmhpcclusters.slack.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">UC Merced HPC Cluster Slack Workspace<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hpc_doc_new/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/amirayuyue/hpc_doc_new" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 University of California, Merced HPC & JupyterHub Documentation.</div></div></div></footer></div>
</body>
</html>